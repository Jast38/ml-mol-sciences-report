@misc{2021ogb,
      title={Open Graph Benchmark: Datasets for Machine Learning on Graphs}, 
      author={Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
      year={2021},
      eprint={2005.00687},
      doi={10.48550/arXiv.2005.00687},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{2018moleculenet,
      title={MoleculeNet: A Benchmark for Molecular Machine Learning}, 
      author={Zhenqin Wu and Bharath Ramsundar and Evan N. Feinberg and Joseph Gomes and Caleb Geniesse and Aneesh S. Pappu and Karl Leswing and Vijay Pande},
      year={2018},
      eprint={1703.00564},
      doi={10.48550/arXiv.1703.00564},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{2017graph2vec,
      title={graph2vec: Learning Distributed Representations of Graphs}, 
      author={Annamalai Narayanan and Mahinthan Chandramohan and Rajasekar Venkatesan and Lihui Chen and Yang Liu and Shantanu Jaiswal},
      year={2017},
      eprint={1707.05005},
      doi={10.48550/arXiv.1707.05005},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{2023longRange,
      title={Long Range Graph Benchmark}, 
      author={Vijay Prakash Dwivedi and Ladislav Rampášek and Mikhail Galkin and Ali Parviz and Guy Wolf and Anh Tuan Luu and Dominique Beaini},
      year={2023},
      eprint={2206.08164},
      doi={10.48550/arXiv.2206.08164},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{2021graphormer,
      title={Do Transformers Really Perform Bad for Graph Representation?}, 
      author={Chengxuan Ying and Tianle Cai and Shengjie Luo and Shuxin Zheng and Guolin Ke and Di He and Yanming Shen and Tie-Yan Liu},
      year={2021},
      eprint={2106.05234},
      doi={10.48550/arXiv.2106.05234},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{2023shortestPathNets,
      title={Shortest Path Networks for Graph Property Prediction}, 
      author={Ralph Abboud and Radoslav Dimitrov and İsmail İlkan Ceylan},
      year={2023},
      eprint={2206.01003},
      doi={10.48550/arXiv.2206.01003},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{2023graphgps,
      title={Recipe for a General, Powerful, Scalable Graph Transformer}, 
      author={Ladislav Rampášek and Mikhail Galkin and Vijay Prakash Dwivedi and Anh Tuan Luu and Guy Wolf and Dominique Beaini},
      year={2023},
      eprint={2205.12454},
      doi={10.48550/arXiv.2205.12454},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{2020deepergcn,
      title={DeeperGCN: All You Need to Train Deeper GCNs}, 
      author={Guohao Li and Chenxin Xiong and Ali Thabet and Bernard Ghanem},
      year={2020},
      eprint={2006.07739},
      doi={10.48550/arXiv.2006.07739},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{2016node2vec,
      title={node2vec: Scalable Feature Learning for Networks}, 
      author={Aditya Grover and Jure Leskovec},
      year={2016},
      eprint={1607.00653},
      doi={10.48550/arXiv.1607.00653},
      archivePrefix={arXiv},
      primaryClass={cs.SI}
}
@misc{2022surveyPretraining,
      title={A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications}, 
      author={Jun Xia and Yanqiao Zhu and Yuanqi Du and Stan Z. Li},
      year={2022},
      eprint={2202.07893},
      doi={10.48550/arXiv.2202.07893},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{2022pretrainingSupervision,
      title={Graph Neural Networks Pretraining Through Inherent Supervision for Molecular Property Prediction},
      author={Benjamin, Roy and Singer, Uriel and Radinsky, Kira},
      year={2022},
      isbn={9781450392365},
      publisher={Association for Computing Machinery},
      address={New York, NY, USA},
      url={https://doi.org/10.1145/3511808.3557085},
      doi={10.1145/3511808.3557085},
      abstract={Recent global events have emphasized the importance of accelerating the drug discovery process. A way to deal with the issue is to use machine learning to increase the rate at which drugs are made available to the public. However, chemical labeled data for real-world applications is extremely scarce making traditional approaches less effective. A fruitful course of action for this challenge is to pretrain a model using related tasks with large enough datasets, with the next step being finetuning it for the desired task. This is challenging as creating these datasets requires labeled data or expert knowledge. To aid in solving this pressing issue, we introduce MISU - Molecular Inherent SUpervision, a unique method for pretraining graph neural networks for molecular property prediction. Our method leapfrogs past the need for labeled data or any expert knowledge by introducing three innovative components that utilize inherent properties of molecular graphs to induce information extraction at different scales, from the local neighborhood of an atom to substructures in the entire molecule. Our empirical results for six chemical-property-prediction tasks show that our method reaches state-of-the-art results compared to numerous baselines.},
      booktitle={Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
      pages={2903–2912},
      numpages={10},
      keywords={drug discovery, ML for healthcare, molecular property prediction},
      location={Atlanta, GA, USA},
      series={CIKM '22}
}
@misc{2014doc2vec,
      title={Distributed Representations of Sentences and Documents}, 
      author={Quoc V. Le and Tomas Mikolov},
      year={2014},
      eprint={1405.4053},
      doi={10.48550/arXiv.1405.4053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{2020graphTransformers,
      title={Graph Transformer Networks}, 
      author={Seongjun Yun and Minbyul Jeong and Raehyun Kim and Jaewoo Kang and Hyunwoo J. Kim},
      year={2020},
      eprint={1911.06455},
      doi={10.48550/arXiv.1911.06455},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{li2019graphTransformer,
      title={Graph Transformer },
      author={Yuan Li and Xiaodan Liang and Zhiting Hu and Yinbo Chen and Eric P. Xing},
      year={2019},
      url={https://openreview.net/forum?id=HJei-2RcK7},
}
@misc{cai2019graphTransformerSequence,
      title={Graph Transformer for Graph-to-Sequence Learning}, 
      author={Deng Cai and Wai Lam},
      year={2019},
      eprint={1911.07470},
      doi={10.48550/arXiv.1911.07470},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{dwivedi2021generalizationTransformer,
      title={A Generalization of Transformer Networks to Graphs}, 
      author={Vijay Prakash Dwivedi and Xavier Bresson},
      year={2021},
      eprint={2012.09699},
      doi={10.48550/arXiv.2012.09699},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{hu2021ogblsc,
      title={OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs}, 
      author={Weihua Hu and Matthias Fey and Hongyu Ren and Maho Nakata and Yuxiao Dong and Jure Leskovec},
      year={2021},
      eprint={2103.09430},
      doi={10.48550/arXiv.2103.09430},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      doi={10.48550/arXiv.1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{tencenc2021Hig,
      title={HIG-GraphClassification},
      author={Yan Wang and Hao Zhang and Jing Yang and Ruixin Zhang and Shouhong Ding},
      year={2021},
      publisher={Github},
      journal={Github repositry},
      howpublished={\url{https://github.com/TencentYoutuResearch/HIG-GraphClassification.git}},
      commit={92a861ce7a753cb397b169dfd1d7dab130d33cc9}
}
@techreport{tencenc2021HigPaper,
      title={Technical Report for OGB Graph Property Prediction},
      author={Yan Wang and Hao Zhang and Jing Yang and Ruixin Zhang and Shouhong Ding},
      month={12},
      year={2021},
      institution={Tencent Youtu Lab},
      address={Shanghai, China},
      abstract={This technical report introduces our solution for two OGB Graph Property
Prediction challenges ogbg-molhiv and ogbg-molpcba},
      url={https://github.com/TencentYoutuResearch/HIG-GraphClassification/blob/main/report/Report_HIG_OGB.pdf}
}