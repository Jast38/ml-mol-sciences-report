@misc{2021ogb,
  title         = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
  author        = {Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
  year          = {2021},
  eprint        = {2005.00687},
  doi           = {10.48550/arXiv.2005.00687},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{2018moleculenet,
  title         = {MoleculeNet: A Benchmark for Molecular Machine Learning},
  author        = {Zhenqin Wu and Bharath Ramsundar and Evan N. Feinberg and Joseph Gomes and Caleb Geniesse and Aneesh S. Pappu and Karl Leswing and Vijay Pande},
  year          = {2018},
  eprint        = {1703.00564},
  doi           = {10.48550/arXiv.1703.00564},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{2017graph2vec,
  title         = {graph2vec: Learning Distributed Representations of Graphs},
  author        = {Annamalai Narayanan and Mahinthan Chandramohan and Rajasekar Venkatesan and Lihui Chen and Yang Liu and Shantanu Jaiswal},
  year          = {2017},
  eprint        = {1707.05005},
  doi           = {10.48550/arXiv.1707.05005},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@misc{2023longRange,
  title         = {Long Range Graph Benchmark},
  author        = {Vijay Prakash Dwivedi and Ladislav Rampášek and Mikhail Galkin and Ali Parviz and Guy Wolf and Anh Tuan Luu and Dominique Beaini},
  year          = {2023},
  eprint        = {2206.08164},
  doi           = {10.48550/arXiv.2206.08164},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{2021graphormer,
  title         = {Do Transformers Really Perform Bad for Graph Representation?},
  author        = {Chengxuan Ying and Tianle Cai and Shengjie Luo and Shuxin Zheng and Guolin Ke and Di He and Yanming Shen and Tie-Yan Liu},
  year          = {2021},
  eprint        = {2106.05234},
  doi           = {10.48550/arXiv.2106.05234},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{2023shortestPathNets,
  title         = {Shortest Path Networks for Graph Property Prediction},
  author        = {Ralph Abboud and Radoslav Dimitrov and İsmail İlkan Ceylan},
  year          = {2023},
  eprint        = {2206.01003},
  doi           = {10.48550/arXiv.2206.01003},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{2023graphgps,
  title         = {Recipe for a General, Powerful, Scalable Graph Transformer},
  author        = {Ladislav Rampášek and Mikhail Galkin and Vijay Prakash Dwivedi and Anh Tuan Luu and Guy Wolf and Dominique Beaini},
  year          = {2023},
  eprint        = {2205.12454},
  doi           = {10.48550/arXiv.2205.12454},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{2020deepergcn,
  title         = {DeeperGCN: All You Need to Train Deeper GCNs},
  author        = {Guohao Li and Chenxin Xiong and Ali Thabet and Bernard Ghanem},
  year          = {2020},
  eprint        = {2006.07739},
  doi           = {10.48550/arXiv.2006.07739},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{2016node2vec,
  title         = {node2vec: Scalable Feature Learning for Networks},
  author        = {Aditya Grover and Jure Leskovec},
  year          = {2016},
  eprint        = {1607.00653},
  doi           = {10.48550/arXiv.1607.00653},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SI}
}
@misc{2022surveyPretraining,
  title         = {A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications},
  author        = {Jun Xia and Yanqiao Zhu and Yuanqi Du and Stan Z. Li},
  year          = {2022},
  eprint        = {2202.07893},
  doi           = {10.48550/arXiv.2202.07893},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{2022pretrainingSupervision,
  title     = {Graph Neural Networks Pretraining Through Inherent Supervision for Molecular Property Prediction},
  author    = {Benjamin, Roy and Singer, Uriel and Radinsky, Kira},
  year      = {2022},
  isbn      = {9781450392365},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3511808.3557085},
  doi       = {10.1145/3511808.3557085},
  abstract  = {Recent global events have emphasized the importance of accelerating the drug discovery process. A way to deal with the issue is to use machine learning to increase the rate at which drugs are made available to the public. However, chemical labeled data for real-world applications is extremely scarce making traditional approaches less effective. A fruitful course of action for this challenge is to pretrain a model using related tasks with large enough datasets, with the next step being finetuning it for the desired task. This is challenging as creating these datasets requires labeled data or expert knowledge. To aid in solving this pressing issue, we introduce MISU - Molecular Inherent SUpervision, a unique method for pretraining graph neural networks for molecular property prediction. Our method leapfrogs past the need for labeled data or any expert knowledge by introducing three innovative components that utilize inherent properties of molecular graphs to induce information extraction at different scales, from the local neighborhood of an atom to substructures in the entire molecule. Our empirical results for six chemical-property-prediction tasks show that our method reaches state-of-the-art results compared to numerous baselines.},
  booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
  pages     = {2903–2912},
  numpages  = {10},
  keywords  = {drug discovery, ML for healthcare, molecular property prediction},
  location  = {Atlanta, GA, USA},
  series    = {CIKM '22}
}
@misc{2014doc2vec,
  title         = {Distributed Representations of Sentences and Documents},
  author        = {Quoc V. Le and Tomas Mikolov},
  year          = {2014},
  eprint        = {1405.4053},
  doi           = {10.48550/arXiv.1405.4053},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{2020graphTransformers,
  title         = {Graph Transformer Networks},
  author        = {Seongjun Yun and Minbyul Jeong and Raehyun Kim and Jaewoo Kang and Hyunwoo J. Kim},
  year          = {2020},
  eprint        = {1911.06455},
  doi           = {10.48550/arXiv.1911.06455},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{li2019graphTransformer,
  title  = {Graph Transformer },
  author = {Yuan Li and Xiaodan Liang and Zhiting Hu and Yinbo Chen and Eric P. Xing},
  year   = {2019},
  url    = {https://openreview.net/forum?id=HJei-2RcK7}
}
@misc{cai2019graphTransformerSequence,
  title         = {Graph Transformer for Graph-to-Sequence Learning},
  author        = {Deng Cai and Wai Lam},
  year          = {2019},
  eprint        = {1911.07470},
  doi           = {10.48550/arXiv.1911.07470},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{mikolov2013distributed,
  title         = {Distributed Representations of Words and Phrases and their Compositionality},
  author        = {Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
  year          = {2013},
  eprint        = {1310.4546},
  doi           = {10.48550/arXiv.1310.4546},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{Perozzi_2014,
  doi       = {10.1145/2623330.2623732},
  url       = {https://doi.org/10.1145%2F2623330.2623732},
  year      = 2014,
  month     = {aug},
  publisher = {{ACM}},
  author    = {Bryan Perozzi and Rami Al-Rfou and Steven Skiena},
  title     = {{DeepWalk}},
  booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining}
}
@misc{dwivedi2021generalizationTransformer,
  title         = {A Generalization of Transformer Networks to Graphs},
  author        = {Vijay Prakash Dwivedi and Xavier Bresson},
  year          = {2021},
  eprint        = {2012.09699},
  doi           = {10.48550/arXiv.2012.09699},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{hu2021ogblsc,
  title         = {OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs},
  author        = {Weihua Hu and Matthias Fey and Hongyu Ren and Maho Nakata and Yuxiao Dong and Jure Leskovec},
  year          = {2021},
  eprint        = {2103.09430},
  doi           = {10.48550/arXiv.2103.09430},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{vaswani2017attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2017},
  eprint        = {1706.03762},
  doi           = {10.48550/arXiv.1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{tencenc2021Hig,
  title        = {HIG-GraphClassification},
  author       = {Yan Wang and Hao Zhang and Jing Yang and Ruixin Zhang and Shouhong Ding},
  year         = {2021},
  publisher    = {Github},
  journal      = {Github repositry},
  howpublished = {\url{https://github.com/TencentYoutuResearch/HIG-GraphClassification.git}},
  commit       = {92a861ce7a753cb397b169dfd1d7dab130d33cc9}
}
@techreport{tencenc2021HigPaper,
  title       = {Technical Report for OGB Graph Property Prediction},
  author      = {Yan Wang and Hao Zhang and Jing Yang and Ruixin Zhang and Shouhong Ding},
  month       = {12},
  year        = {2021},
  institution = {Tencent Youtu Lab},
  address     = {Shanghai, China},
  abstract    = {This technical report introduces our solution for two OGB Graph Property
                 Prediction challenges ogbg-molhiv and ogbg-molpcba},
  url         = {https://github.com/TencentYoutuResearch/HIG-GraphClassification/blob/main/report/Report_HIG_OGB.pdf}
}
@misc{wu2023DeepChem,
  title        = {DeepChem},
  author       = {Zhenqin Wu et al.},
  year         = {2023},
  publisher    = {Github},
  journal      = {Github repositry},
  howpublished = {\url{https://github.com/deepchem/deepchem}}
}
  @misc{kreuzer2021rethinking,
  title         = {Rethinking Graph Transformers with Spectral Attention},
  author        = {Devin Kreuzer and Dominique Beaini and William L. Hamilton and Vincent Létourneau and Prudencio Tossou},
  year          = {2021},
  eprint        = {2106.03893},
  archiveprefix = {arXiv},
  doi           = {10.48550/arXiv.2106.03893},
  primaryclass  = {cs.LG}
}
@misc{mialon2021graphit,
  title         = {GraphiT: Encoding Graph Structure in Transformers},
  author        = {Grégoire Mialon and Dexiong Chen and Margot Selosse and Julien Mairal},
  year          = {2021},
  eprint        = {2106.05667},
  archiveprefix = {arXiv},
  doi           = {10.48550/arXiv.2106.05667},
  primaryclass  = {cs.LG}
}
@misc{wu2022representing,
  title         = {Representing Long-Range Context for Graph Neural Networks with Global Attention},
  author        = {Zhanghao Wu and Paras Jain and Matthew A. Wright and Azalia Mirhoseini and Joseph E. Gonzalez and Ion Stoica},
  year          = {2022},
  eprint        = {2201.08821},
  archiveprefix = {arXiv},
  doi           = {10.48550/arXiv.2201.08821},
  primaryclass  = {cs.LG}
}
@misc{kalyan2021ammus,
  title         = {AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing},
  author        = {Katikapalli Subramanyam Kalyan and Ajit Rajasekharan and Sivanesan Sangeetha},
  year          = {2021},
  eprint        = {2108.05542},
  archiveprefix = {arXiv},
  doi           = {10.48550/arXiv.2108.05542},
  primaryclass  = {cs.CL}
}
@article{belkin2003laplacian,
  author  = {Belkin, Mikhail and Niyogi, Partha},
  journal = {Neural Computation},
  title   = {Laplacian Eigenmaps for Dimensionality Reduction and Data Representation},
  year    = {2003},
  volume  = {15},
  number  = {6},
  pages   = {1373-1396},
  doi     = {10.1162/089976603321780317}
}
@misc{dwivedi2022graph,
  title         = {Graph Neural Networks with Learnable Structural and Positional Representations},
  author        = {Vijay Prakash Dwivedi and Anh Tuan Luu and Thomas Laurent and Yoshua Bengio and Xavier Bresson},
  year          = {2022},
  eprint        = {2110.07875},
  archiveprefix = {arXiv},
  doi           = {10.48550/arXiv.2110.07875},
  primaryclass  = {cs.LG}
}
@article{Wittmann2009reconstruction,
  author  = {Wittmann, Dominik and Schmidl, Daniel and Blöchl, Florian and Theis, Fabian},
  year    = {2009},
  month   = {09},
  pages   = {3826-3838},
  title   = {Reconstruction of graphs based on random walks},
  volume  = {410},
  journal = {Theoretical Computer Science},
  doi     = {10.1016/j.tcs.2009.05.026}
}
@misc{epasto2019embedding,
  title        = {DeepChem},
  author       = {Alessandro Epasto},
  year         = {2019},
  publisher    = {Google AI},
  journal      = {Google AI Blog},
  howpublished = {\url{https://ai.googleblog.com/2019/06/innovations-in-graph-representation.html}}
}
@misc{our_repo,
  title        = {Graph-Embedding},
  author       = {Christian Staib and Jannick Stuby and Lukas Zeil},
  year         = {2023},
  publisher    = {Github},
  journal      = {Github repositry},
  howpublished = {\url{https://github.com/chrishaxalot/graph-embedding.git}}
}
@misc{zaheer2021bigbird,
      title={Big Bird: Transformers for Longer Sequences}, 
      author={Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
      year={2021},
      eprint={2007.14062},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      doi={https://doi.org/10.48550/arXiv.2007.14062}
}
@misc{bresson2018GatedGCN,
      title={Residual Gated Graph ConvNets}, 
      author={Xavier Bresson and Thomas Laurent},
      year={2018},
      eprint={1711.07553},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      doi={10.48550/arXiv.1711.07553}
}
@misc{choromanski2022performer,
      title={Rethinking Attention with Performers}, 
      author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
      year={2022},
      eprint={2009.14794},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
      doi={10.48550/arXiv.2009.14794}
}
