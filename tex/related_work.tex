\section{Preliminary}

As we started our project examining DeepChem, we first looked into handcrafted graph to vector methods.

Later we looked into graph machine learning.

\subsection{Open Graph Benchmark}
Open Graph Benchmark(OGB)~\cite{2021ogb} is a framework for benchmarking machine learning on graphs, providing large-scale, real world datasets as well as an easy to use software framework for loading  datasets and evaluating model performance.
For comparing model performances, OGB provides leaderboards for different datasets and challenges, split into the sections Node Property Prediction, Link Property Prediction and Graph Property Prediction, as well as a large-scale challenge~\cite{hu2021ogblsc} leaderboard, which aids comparison of performance on very large graphs.

Our main focus will be Graph Property Prediction, mainly on the datasets ogb-molhiv and ogb-molpcba. Both these datasets are derived from MoleculeNet and respectively provide a large amount of molecules for molecular property prediction. Molecules are represented by graphs, where atoms are the nodes and corresponding chemical bonds are represented by the edges. Node features are represented as a 9-dimensional vector, edge feature vectors are 3-dimensional, the respective value ranges can be seen in \autoref{node_feature_list} and \autoref{edge_features}.
For ogb-molhiv the model is evaluated using ROC-AUC, for ogb-molpcba Average Precision is used.

\begin{table}
    \centering
    \caption{Node feature vector}
    \label{node_feature_list}
    \begin{tabular}{c || l| p{6cm} |}
        Index & Feature                       & Value Range                                                  \\
        \hline
        \hline
        0     & atom number (H excluded, C=5) & 1 - 119                                                      \\
        1     & chirality                     & unspecified, Tetrahedral CW, Tetrahedral CCW, other and misc \\
        2     & degree                        & 0 - 10, misc                                                 \\
        3     & formal charge                 & -5 - 5, misc                                                 \\
        4     & number of H-atoms             & 0 - 8, misc                                                  \\
        5     & number of radicals            & 0 - 4, misc                                                  \\
        6     & hybridization                 & SP, SP2, SP3, SP3D, SP3D2, misc                              \\
        7     & is aromatic                   & true, false                                                  \\
        8     & is in ring                    & true, false                                                  \\
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Edge feature vector}
    \label{edge_features}
    \begin{tabular}{l || l p{6cm} |}
        Index & Feature       & Value Range                            \\
        \hline
        \hline
        0     & bond type     & single, double, triple, aromatic, misc \\
        1     & bond stereo   & none, z, e, cis, trans, any            \\
        2     & is conjugated & true, false
    \end{tabular}
\end{table}


We additionally used ogb-molfreesolv as a smaller dataset which is also derived from MoleculeNet. FreeSolv contains SMILES strings of molecules from the Free Solvation Database and consists of "experimental and calculated hydration free energy of small molecules in water"~\cite{2018moleculenet}. The model is evaluated using Root Mean Square Error.

We decided to use OGB as a benchmarking framework instead of the MoleculeNet benchmark results, because OGB submissions are much more versatile and active, containing results from many research teams with the latest submission for ogb-molhiv dating to May 2022, contrary to the MoleculeNet results which were provided by the authors of the paper only and are dated to January 2018.


\subsection{Graph Embedding}
TODO: Add information on graph embedding relevant to problem domain

\subsection{node2vec}
\todo{\tiny 28.32\% AI GPT}

Node2Vec, presented by Grover and Leskovec in 2016, is a framework for learning low-dimensional representations of nodes in large-scale networks~\cite{2016node2vec}. The goal of node2vec is to learn embeddings that capture the structural information of nodes, such as their connectivity patterns, in a way that is useful for downstream tasks, such as node classification, clustering, and link prediction.

Node2vec builds on the skip-gram model used in natural language processing, which learns word embeddings by predicting the context of a given word. Similarly, node2vec learns node embeddings by training a neural network to predict the context of a given node, where the context of a node is defined as the set of nodes that are likely to appear in random walks starting from that node.

To generate random walks, node2vec uses a biased random walk strategy that balances the trade-off between exploring the network and exploiting the local neighborhood of a node. The random walk starts at a given node choosing for each step to either revisit the previous node or move to the next one based on two parameters: the in-out degree bias parameter $p$ and the return parameter $q$, where $p$ controls the likelihood of staying in the same community, while $q$ controls the likelihood of visiting a node that is far away from the current one. By tuning these parameters, node2vec can generate different types of random walks that capture different types of structural information in the network.

Once the random walks are generated, node2vec uses the skip-gram model to learn node embeddings by maximizing the likelihood of predicting the context nodes given the current node. The skip-gram model is trained using negative sampling, which samples negative nodes that are not in the context set and adjusts the embedding vectors to minimize the difference between the predicted and actual context sets.

Node2vec has several advantages over previous methods for network embedding. While being scalable to large networks and being able to handle heterogeneous networks with multiple types of nodes and edges, it can also capture both the local and global structure of the network by tuning the $p$ and $q$ parameters. Additionally, it can handle noisy or incomplete networks by incorporating information from the context nodes. Finally, it can be applied to a wide range of downstream tasks, such as link prediction, node classification, and community detection.

\subsubsection{doc2vec}
Doc2vec is a widely used method for obtaining distributed representations of documents as continuous vectors in a low-dimensional space, also known as paragraph embeddings. Doc2vec, introduced by Le and Mikolov~\cite{2014doc2vec}, extends the popular Word2vec algorithm~\cite{mikolov2013distributed} to the domain of entire documents.

Doc2vec learns a vector representation for each document in a corpus by utilizing a neural network architecture called Paragraph Vector. The architecture is similar to that of Word2vec but includes an additional vector representation for each document. There are two ways to obtain document vectors: Distributed Memory (DM) and Distributed Bag of Words (DBOW). 

In the Distributed Memory model, the document vector is trained to predict the next word in a randomly sampled sequence of words, with the document vector and a window of surrounding words as input. The document vector is then updated alongside the word vectors during backpropagation. This model captures the overall topic and meaning of the document, as it has access to all words in the document.

In contrast, the Distributed Bag of Words model disregards the order of words and only utilizes the occurrence information of the words in the document. The input to the model is a bag of words from a randomly sampled window, and the objective is to predict the words in the window. The document vector is trained alongside the word vectors, and it aims to represent the document's overall semantic content.

In both models, the document vector is learned by optimizing a loss function, typically the negative log-likelihood of predicting the target words. The resulting document vectors are continuous, dense, and low-dimensional, making them suitable for various downstream tasks, such as document classification, clustering, and information retrieval.

Doc2vec has been shown to outperform other methods for document embeddings in several benchmark datasets and downstream tasks. The embeddings capture the semantics and meaning of the documents and can be used to infer similarities between documents, making them useful for document clustering and information retrieval.

\subsubsection{graph2vec}
TODO: explain graph2vec \cite{2017graph2vec} approach with respect to problem domain

Graph2Vec will be used by us as a benchmark.

\subsection{Transformers}
TODO: add Transformer~\cite{vaswani2017attention} section: add explanation, formulas, visualization

In 2017 Vaswani et al. proposed a new network architecture for NLP problems, called the Transformer~\cite{vaswani2017attention}. The Transformer architecture is build of many layers, each containing a self-attention module and a position-wise feed-forward network. Self-attention is then calculated as described in~\cite{2021graphormer}:

\begin{align}
    Q = HW_Q,\quad K = HW_K,\quad V = HW_V,\label{eqn:attention-alpha}\\
    A = \frac{QK^\top}{\sqrt{d_K}}, \quad \attn{H} = \softmax{A}V,\label{eqn:attention-alpha2}
\end{align}

Here $Q$, $K$, and $V$ are calculated by multiplying the input $H$ of the attention module with the respective matrices $W_Q\in\Rbb^{d\times d_K}, W_K\in\Rbb^{d\times d_K}$ and $ W_V\in\Rbb^{d\times d_V}$ which then represent Queries, Keys and Values of the attention module. $d$ denotes the hidden dimensions, which in case of single-head self attention is the same as $d_K$ and $d_V$. This leads to \autoref{eqn:attention-matrix}

\subsubsection{Graphormer}
As an adaption of Transformers~\cite{vaswani2017attention} for graph representation learning, Graphormer~\cite{2021graphormer} was presented by Ying et al., providing a performant Transformer model, excelling in various graph representation learning tasks, particularly for complex structures.

Graphormer achieves such good results by introducing three types of encodings as a bias to the self-attention mechanism used in Transformer architecture. While Transformer models work by computing weighted sum of input features based on attention scores between different tokens during self-attention, Graphormer incorporates structural information of different nodes in pairwise relation to each other into the self-attention mechanism, thus increasing the information encapsuled in the self-attention module, which is held by graph structures and features present.

To capture information about the importance of a node in a given network, Graphormer uses a measure called node centrality by utilizing degree centrality of a given node. This is done by introducing learnable embedding vectors, one for the degree of ingoing edges and one for the outgoing ones. Those vectors are scalars which are added to the features of each node, so node importance and semantic correlation are respected during self attention. Those scalars are indexed from two vectors according to the respective degrees as seen in \autoref{fig:graphormer_centr_enc}.

The other two encodings are added as bias to the Attention Module used in Transformers as described in \autoref{fig:graphormer_att} and the following equation.
\begin{equation}
    A_{ij} = \frac{(h_i W_Q)(h_j W_K)^T}{\sqrt{d}} + b_{\Phi (v_i, v_j)} + c_{ij}
    \label{eqn:attention-matrix}
\end{equation}

The bias $b_{\Phi (v_i, v_j)}$ is produced by the so called spatial encoding, which captures positional information for each node with respect to the network. This is achieved by using the distance of the shortest path between two connected nodes, described by the function $\Phi$, combined with a learnable vector which is indexed according to $\Phi$. The resulting scalar is added as bias in the self-attention module. 

By doing that, the model can attend to a more relevant subset of the network according to the task. An example given in the paper is the possibility to increase attention to nodes surrounding a given node instead of nodes which lay further away.

Finally Graphormer introduces a method to include edge information in the model by adding an the additional bias term $c_{ij}$ to the attention module. This bias is created by averaging the dot-products of edge features ($x_{e_n}$) and a learnable scalar ($(w_n^E)^T$), which is indexed according to the distance of the edge to the start node dor each edge along the shortest path between each two connected nodes.

\begin{equation}
    c_{ij} = \frac{1}{N} \sum_{n=1}^{N} x_{e_n}(w_n^E)^T
\end{equation}

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{tex/res/graphormer_centr_enc.png}
    \caption{Centrality Encoding: learnable embedding vectors are added to feature vectors of nodes}
    \label{fig:graphormer_centr_enc}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{tex/res/graphormer_attention.png}
    \caption{Spatial Encoding (left) and Edge Encoding (right) added as bias to Attention Module}
    \label{fig:graphormer_att}
\end{figure}

\subsubsection{GraphGPS}
L. Ramp√°sek et al. introduced GraphGPS in their paper 'Recipe for a General, Powerful, Scalable Graph Transformer.'~\cite{2023graphgps} The objective of this model recipe is to provide a foundation for incorporating structural and positional information into node and edge features, while maintaining the flexibility of the model that implements these features.

GraphGPS serves as a preprocessor for the actual model learning on the dataset. The authors tested GraphGPS with several models, such as Transformer, Performer, BigBird, Gine, and more. They designed it to allow models that permit only Node Features, known as the Global Attention layer, as well as models that permit both Node and Edge Features, known as the MPNN layer. Both layers can also be used in combination to integrate the distinct advantages of two models.

The authors categorized structural and positional information into two types since the distance between nodes, although indicating the graph's structure, is insufficient to capture structural similarities. They incorporated this information by utilizing structural and positional encodings (SE and PE), which are divided into Local, Global, and Relative categories. Local and Global encodings are implemented as node features, while Relative encodings are implemented as edge features.

\subsubsection{Heterogeneous Interpolation on Graphs}
A winning entry for the dataset ogb-molpcba, called HIG-GraphClassification~\cite{tencenc2021Hig,tencenc2021HigPaper} was submitted to the OGB leaderboard in 2021 by Wang et al., providing an implementation and a brief technical report describing the research. By using their method combined with Graphormer, they were able to achieve better average precision than all previous submissions.

The report introduces heterogeneous interpolation, which is done by dropping the feature vectors of several randomly selected nodes and replacing them by the interpolated feature mix of all neighboring nodes.
By using a mixing ratio, the influence of each neighbors features can be adapted. To account for possible information loss, KL-Divergence constraint loss is added. By doing that, the distributions of two identical graphs remain similar after interpolating some feature vectors. The general idea was visualized by Wang et al. in \autoref{fig:hig_figure}.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.3]{tex/res/hig_figure.png}
    \caption{Visualization of HIG-GraphClassification by Wang et al., \tiny{source: \cite{tencenc2021HigPaper}}}
    \label{fig:hig_figure}
\end{figure}

We combine the idea of interpolating feature vectors of neighboring nodes with GraphGPS to test performance with different embedding methods.