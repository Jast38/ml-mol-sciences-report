\section{Preliminary}

As we started our project examining DeepChem, we first looked into handcrafted graph to vector methods.

Later we looked into graph machine learning.

\subsection{Open Graph Benchmark}
Open Graph Benchmark(OGB)~\cite{2021ogb} is a framework for benchmarking machine learning on graphs, providing large-scale, real world datasets as well as an easy to use software framework for loading  datasets and evaluating model performance.
For comparing model performances, OGB provides leaderboards for different datasets and challenges, split into the sections Node Property Prediction, Link Property Prediction and Graph Property Prediction, as well as a large-scale challenge~\cite{hu2021ogblsc} leaderboard, which aids comparison of performance on very large graphs.

Our main focus will be Graph Property Prediction, mainly on the datasets ogb-molhiv and ogb-molpcba. Both these datasets are derived from MoleculeNet and respectively provide a large amount of molecules for molecular property prediction. Molecules are represented by graphs, where atoms are the nodes and corresponding chemical bonds are represented by the edges. \todo{\tiny add input node features (9-dimensional), perhaps table?}

For ogb-molhiv the model is evaluated using ROC-AUC, for ogb-molpcba Average Precision is used.

We additionally used ogb-molfreesolv as a smaller dataset which is also derived from MoleculeNet. FreeSolv contains SMILES strings of molecules from the Free Solvation Database and consists of "experimental and calculated hydration free energy of small molecules in water" \todo{\tiny add citation see moleculeNet paper}. The model is evaluated using Root Mean Square Error.

We decided to use OGB as a benchmarking framework instead of the MoleculeNet benchmark results, because OGB submissions are much more versatile and active, containing results from many research teams with the latest submission for ogb-molhiv dating to May 2022, contrary to the MoleculeNet results which were provided by the authors of the paper only and are dated to January 2018. 


\subsection{Graph Embedding}
TODO: Add information on graph embedding relevant to problem domain

\subsubsection{doc2vec}
TODO: explain doc2vec according to \cite{2014doc2vec} approach with respect to problem domain, possibly move to another section (paragraph/word embedding)

\subsubsection{graph2vec}
TODO: explain graph2vec \cite{2017graph2vec} approach with respect to problem domain

\subsection{Graph Transformers}
TODO: add information for Graph Transformers \cite{2020graphTransformers} \cite{li2019graphTransformer} \cite{cai2019graphTransformerSequence} \cite{dwivedi2021generalizationTransformer} and their usage for the problem domain and OGB leaderboards \todo{\tiny remove this section and merge with Graphormer Section/replace by general Transformer}

With the success of Transformers~\cite{vaswani2017attention} for NLP problems \todo{\tiny cite success information}, Dwivedi et al. introduced Graph Transformers as a generalization of Transformers usable for arbitrary graphs~\cite{dwivedi2021generalizationTransformer} 

\subsubsection{Graphormer}
As an adaption of Transformers~\cite{vaswani2017attention} for graph representation learning, Graphormer~\cite{2021graphormer} was presented by Ying et al., providing a performant Transformer model, excelling in various graph representation learning tasks.

Graphormer achieves such good results by introducing three types of encodings to incorporate structural information of graphs in the feature vectors of nodes and edges. 

To capture information about the importance of a node in a given network, Graphormer uses a measure called node centrality by utilizing degree centrality of a given node. This is done by introducing two new vectors for each node, one for the degree of ingoing edges and one for the outgoing ones. Those vectors are learnable scalars which are added to the node features, so node importance and semantic correlation are respected during self attention. \todo{\tiny add more information about learnable scalars, add general transforer structure to explain queries and keys}

The second encoding, called spatial encoding, is used to capture positional information for each node with respect to the network. This is achieved by using the distance of the shortest path between connected nodes and, once again, using a learnable scalar which is added as bias in the self-attention module. By doing that, the model can attend to a more relevant subset of the network according to the task. An example given in the paper is the possibility to increase attention to nodes surrounding a given node instead of nodes which lay further away.

Finally Graphormer introduces a method to include edge information in the model by adding an additional bias term to the attention module. This bias is created by averaging the dot-products of edge features and a learnable scalar of the edges describing the shortest path between each two connected nodes. \todo{\tiny perhaps add equations according to Transformer and Graphormer paper} 

\subsubsection{GraphGPS}
L. Ramp√°sek et al. introduced GraphGPS in their paper 'Recipe for a General, Powerful, Scalable Graph Transformer.'~\cite{2023graphgps} The objective of this model recipe is to provide a foundation for incorporating structural and positional information into node and edge features, while maintaining the flexibility of the model that implements these features.

GraphGPS serves as a preprocessor for the actual model learning on the dataset. The authors tested GraphGPS with several models, such as Transformer, Performer, BigBird, Gine, and more. They designed it to allow models that permit only Node Features, known as the Global Attention layer, as well as models that permit both Node and Edge Features, known as the MPNN layer. Both layers can also be used in combination to integrate the distinct advantages of two models.

The authors categorized structural and positional information into two types since the distance between nodes, although indicating the graph's structure, is insufficient to capture structural similarities. They incorporated this information by utilizing structural and positional encodings (SE and PE), which are divided into Local, Global, and Relative categories. Local and Global encodings are implemented as node features, while Relative encodings are implemented as edge features.

\subsubsection{Heterogeneous Interpolation on Graphs}
A winning entry for the dataset ogb-molpcba, called HIG-GraphClassification~\cite{tencenc2021Hig,tencenc2021HigPaper} was submitted to the OGB leaderboard in 2021 by Wang et al., providing an implementation and a brief technical report describing the research. By using their method combined with Graphormer, they were able to achieve better average precision than all previous submissions. 

The report introduces heterogeneous interpolation, which is done by clearing dropping the feature vector of several randomly selected nodes and replacing those feature vectors with the interpolated feature mix of all neighboring nodes.
By using a mixing ratio, the influence of each neighbors features can be adapted. To account for possible information loss, KL-Divergence constraint loss ist added. By doing that, the distributions of two identical graphs remain similar after interpolating some feature vectors. The general idea was visualized by Wang et al. in \ref*{hig_figure}.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.3]{tex/res/hig_figure.png}
    \label{hig_figure}
    \caption{Visualization of HIG-GraphClassification by Wang et al., taken from~\cite{tencenc2021HigPaper}}
\end{figure}

We combine the idea of interpolating feature vectors of neighboring nodes with GraphGPS to test performance with different embedding methods. 