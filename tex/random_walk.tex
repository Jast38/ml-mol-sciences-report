\subsection{Walk based graph embeddings}

One of the primary challenges in graph machine learning, as opposed to text-based machine learning, is the multidimensionality of graphs. Each node can have varying numbers of neighbors and different features and feature types. Walk based embeddings reduce this multidimensionality.

For our walk based embedding we focused on the embedding of whole graphs, as a similar method to embed nodes already exists (see \cite{2016node2vec}). The downstream task after the embedding of the graph is either regression or classification. For this we use sklearn.

\subsubsection{walks to vector}
A walk on graph is a unambiguously sequence of vertices and edges, where each vertex is connected to the next vertex by an edge. Given a graph $g$, we can write a walk as list of nodes (denoted by their id), e.g. $(0, 2, 3)$ is a walk that start from the node with the id $0$, goes to $2$ and end at $3$. For a non-trivial graph there a many different possible walks, therefore we can create a set of walks $w$ that all walk on the same graph. At this moment the walk doesn't contain any information relevant for our task, for this we need to substitute the node ids with the corresponding features. We substitute each id with the feature value prefixed with index of the feature separated by an otherwise unused character. For this we require that the features of a node are ordered. Therefore our schema for a feature in the walk is FeatureIndex\_FeatureValue. We use this prefix in order to differentiate between same values of different feature types.

\begin{figure}[ht!]
    \begin{minipage}{\linewidth}
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node[draw=red] (A) at (0,0) {0\\(0, 2, 2)};
                    \node (B) at (0,4) {1\\(7, 0, 4)};
                    \node (C) at (2.5,4) {2\\(2, 0, 4)};
                    \node[draw=red] (D) at (2.5,2) {3\\(8, 3, 9)};
                    \node (E) at (5, 0) {4\\(3, 4, 7)};
                    \node[draw=red] (F) at (5,4) {5\\(2, 1, 7)} ;
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (B);
                    \path [-] (B) edge node {} (C);
                    \path [-, draw=red] (A) edge node {} (D);
                    \path [-] (D) edge node {} (C);
                    \path [-] (A) edge node {} (E);
                    \path [-] (D) edge node {} (E);
                    \path [-, draw=red] (D) edge node {} (F);
                    \path [-] (C) edge node {} (F);
                    \path [-] (E) edge node {} (F);
                \end{scope}
            \end{tikzpicture}
        \end{minipage}%
        \begin{minipage}{.5\linewidth}
            \centering
            (0, 3, 5) \\
            $\Downarrow$ \\
            (0\_0 1\_2 2\_2) (0\_8 1\_3 2\_9) (0\_2 1\_1 2\_7) \\
            $\Downarrow$ \\
            0\_0 1\_2 2\_2 0\_8 1\_3 2\_9 0\_2 1\_1 2\_7
        \end{minipage}
    \end{minipage}
    \caption[short]{Example of graph on which a walk is done that is converted into a sentence}
    \label{figure:example_walk_to_sentence}
\end{figure}

Lets talk about a concrete example. In the left side of figure \ref{figure:example_walk_to_sentence} is an undirected graph with six vertices given. Each vertex has three numerical features. These features are given as list of integers below the node id. The red marked nodes form a walk from node 0 to node 5. On the right side of the figure, we listed the steps needed for each walk to generate the document needed for the downstream training: First we generate a walk, then we substitute the node id with the prefixed features and at last we concatenate the features into a sentence.

After this overview, we can formalize this method. Given a set of graphs $G = \{g_i \mid i \in 0 \dots n \}$, where $n$ is the number of graphs, we can generate a set of sets of walks $W = \{\ w_i \mid i \in 0 \dots n  \}$. Now we create a set $W'$ where for each walk we substitute the node id with the features of the node. If we view each walk as sentence we get a set of documents (multiple sentences). These documents are feed into a text embedding method.

\begin{minipage}{\linewidth}
    \begin{algorithm}[H]
        %\SetKwSty{text}
        \DontPrintSemicolon
        \SetArgSty{text}
        \SetProgSty{text}
        \SetKw{KwIn}{in}

        \SetKwProg{Fn}{def}{}{}

        \Fn{get\_vectors(graphs)}{
            documents = [ ]\;
            \ForAll{graph \KwIn graphs}{
                walks = generate\_walks(graph)\;
                document = walks\_to\_document(walks)\;
                documents.append(document)\;
            }

            model = Text\_Embedding\_Model()\;
            model.fit(documents)\;
            \Return model.get\_document\_vectors()\;
        }

        \caption{basic idea of our walk based embedding}
        \label{algorithm:basic_idea_walk_to_vector}
    \end{algorithm}
\end{minipage}

Pseudocode for our first implementation of our graph embedding method is given in algorithm \ref{algorithm:basic_idea_walk_to_vector}. We identified two ways to influence the embedding in a meaningful way:
\begin{enumerate}
    \item
     change the way walks are generated

    \item
     change the text embedding model
\end{enumerate}

\subsubsection{change the way walks are generated}
To accurately depict the neighborhood of a vertex, it is essential to represent the neighborhood within the generated document. This can be achieved when every combination of adjacent vertices is represented in the document multiple times.

We have considered two approaches for generating walks: All Pair Shortest Paths (APSP) and Random Walks. Each method possesses distinct advantages and drawbacks. APSP tends to have more hotspots of frequently visited graphs, which may introduce bias in the document; however, it generates similar walks for structurally analogous graphs. On the other hand, Random Walks can be set to a specific length and do not exhibit the same issue with frequently visited graphs. Nevertheless, they do not guarantee the generation of similar walks for structurally similar graphs.

One reason to consider using graph walks for analysis is that, given a sufficient number of walks over a graph, it becomes possible to reconstruct the original graph to some degree. These walks therefore provide insight into the graph's structure, capturing information about the relationships between vertices and the paths connecting them. By aggregating data from numerous walks, a comprehensive representation of the graph can be formed, enabling its reconstruction for further evaluation.

\paragraph{Reconstructing a graph from walks}
With an adequate quantity of walks over a graph, one can partially reconstruct the graph. However, there are limitations: For example, distinguishing between 'line' and 'circle' subgraphs is not always possible without any specific preparation of the graph. To illustrate this, refer to Figure \ref{figure:random_walk_cant_differentiate}. It is evident that each potential walk over one two graphs can be accommodated within the other graph as well.

\begin{figure}[ht!]
    \begin{minipage}{\linewidth}
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node (A) at (0,0) {0};
                    \node (B) at (0,4) {0};
                    \node (C) at (5,4) {0};
                    \node (D) at (5,0) {0};
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (B);
                    \path [-] (A) edge node {} (D);
                    \path [-] (C) edge node {} (B);
                    \path [-] (C) edge node {} (D);
                \end{scope}
            \end{tikzpicture}
        \end{minipage}%
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node (A) at (0,0) {0};
                    \node (B) at (0,4) {0};
                    \node (C) at (5,4) {0};
                    \node (D) at (5,0) {0};
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (B);
                    \path [-] (A) edge node {} (D);
                    \path [-] (C) edge node {} (D);
                \end{scope}
            \end{tikzpicture}
        \end{minipage}%
    \end{minipage}
    \caption[short]{Example of two graphs whose walks can not be be differentiated}
    \label{figure:random_walk_cant_differentiate}
\end{figure}

Similar, i the case of regular graphs (i.e., graphs wherein all vertices exhibit the same degree), nodes with identical features tend to appear indistinguishable during walks. Consequently, due to these inherent constraints, it was not anticipated that walk-based whole graph embeddings would yield exceptional results.

\paragraph{change the text embedding model}
Throughout the entirety of our project, we employed Doc2Vec\cite{2014doc2vec} for transforming our walks into vector representations. Doc2Vec facilitates the training of a custom model using our specific vocabulary, allowing for the embedding of documents into vectors within this vocabulary. Despite our efforts to identify an alternative document-to-vector model that was user-friendly, we were unable to locate a suitable candidate.

\subsubsection{results}
Given the substantial amount of data generated by the random walks, which in turn resulted in a significant volume of training data for Doc2Vec, our focus was directed towards the smaller datasets from the Open Graph Benchmark (OGB). Specifically, we concentrated on ogb-molfreesolv, a regression dataset comprising 642 graphs. As previously noted, we were aware of the limitations associated with our methodology. Consequently, we established a baseline by comparing the vectors generated through our approach with randomly generated ones.

ogbg-molfreesolv
all pair shortest path:6.528969941013406
[6.609125555671113, 6.57278283862703, 6.456680877236287, 6.539184198294062, 6.593562123437148, 6.417587074655182, 6.570877169599866, 6.478911030725958, 6.496761022461019, 6.5542275194263935]

\subsubsection{change the text embedding model}