\section{Our approach: Paths2Vec}

We decided to implement a graph machine learning method ourselves. Recognizing that it would be unlikely for us to devise a groundbreaking approach, we aimed to explore the simplest method we could imagine: a walk-based embedding.

For our walk-based embedding we focused on the embedding of whole graphs, as a similar method to embed nodes already exists\cite{2016node2vec}. One of the primary challenges in graph machine learning as opposed to text-based machine learning is the multidimensionality of graphs. Each node can have varying numbers of neighbors and different features and feature types. Walk based embeddings reduce this multidimensionality as the generated walk is a 2d structure.

Given a sufficient number of walks over a graph, it becomes possible to reconstruct the original graph to some degree\cite{Wittmann2009reconstruction}. These walks therefore provide insight into the graph, capturing information about the relationships between vertices and the paths connecting them. Aggregated data from numerous walks therefore contains information about the the graph.

Our method, which we named Paths2Vec, can be described at a high level as follows: Given a graph, we generate a list of walks over the graph, in which the node ids are substituted with the corresponding node features. This process results in a document-like representation of the graph. Utilizing natural language processing techniques, we then transform this 'document' into a vector representation. These vector representations can subsequently be employed to train a model capable of predicting various graph characteristics.

Our implementations and code changes to existing methods can be found in our GitHub Repository~\cite{our_repo}.

\subsection{Walk based graph embeddings}
A walk on graph is a sequence of vertices and edges, where each vertex is connected to the next vertex by an edge. Given a graph $g$, we can write a walk as list of nodes (denoted by their id). For example, $(0, 2, 3)$ is a walk that start from the node with the id $0$, goes to $2$ and ends at $3$. For a non-trivial graph there a many different possible walks, therefore we can create a set of walks $w$ that all walk on the same graph. At this moment the walk doesn't contain any information relevant for our task, for this we need to substitute the node ids with the corresponding features. We substitute each id with the feature value prefixed with index of the feature separated by an otherwise unused character. For this we require that the features of a node are ordered. Therefore our schema for a feature in the walk is FeatureIndex\_FeatureValue. We use this prefix in order to differentiate between same values of different feature types.

\begin{figure}[ht!]
    \begin{minipage}{\linewidth}
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node[draw=red] (A) at (0,0) {0\\(0, 2, 2)};
                    \node (B) at (0,4) {1\\(7, 0, 4)};
                    \node (C) at (2.5,4) {2\\(2, 0, 4)};
                    \node[draw=red] (D) at (2.5,2) {3\\(8, 3, 9)};
                    \node (E) at (5, 0) {4\\(3, 4, 7)};
                    \node[draw=red] (F) at (5,4) {5\\(2, 1, 7)} ;
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (B);
                    \path [-] (B) edge node {} (C);
                    \path [-, draw=red] (A) edge node {} (D);
                    \path [-] (D) edge node {} (C);
                    \path [-] (A) edge node {} (E);
                    \path [-] (D) edge node {} (E);
                    \path [-, draw=red] (D) edge node {} (F);
                    \path [-] (C) edge node {} (F);
                    \path [-] (E) edge node {} (F);
                \end{scope}
            \end{tikzpicture}
        \end{minipage}%
        \begin{minipage}{.5\linewidth}
            \centering
            (0, 3, 5) \\
            $\Downarrow$ \\
            (0\_0 1\_2 2\_2) (0\_8 1\_3 2\_9) (0\_2 1\_1 2\_7) \\
            $\Downarrow$ \\
            0\_0 1\_2 2\_2 0\_8 1\_3 2\_9 0\_2 1\_1 2\_7
        \end{minipage}
    \end{minipage}
    \caption[short]{Example of graph on which a walk is done that is converted into a sentence}
    \label{figure:example_walk_to_sentence}
\end{figure}

Lets talk about a concrete example. In the left side of figure \ref{figure:example_walk_to_sentence} is an undirected graph with six vertices given. Each vertex has three numerical features. These features are given as list of integers below the node id. The red marked nodes form a walk from node 0 to node 5. On the right side of the figure, we listed the steps needed for each walk to generate the document needed for the downstream training: First we generate a walk, then we substitute the node id with the prefixed features and at last we concatenate the features into a sentence.

Now we can formalize this method. Given a set of graphs $G = \{ g_i \mid i \in 0 \dots n \}$, where $n$ is the number of graphs, we can generate a set of sets of walks $W = \{ w_i \mid i \in 0 \dots n \}$. Next, we create a new set $W'$ in which we substitute the node ID with the node features for each walk. If we consider each walk as a sentence, we obtain a set of documents (comprising of multiple sentences). These documents are then fed into a text embedding method. The pseudocode of our graph embedding method is provided in algorithm \ref{algorithm:basic_idea_walk_to_vector}.

\begin{minipage}{\linewidth}
    \begin{algorithm}[H]
        %\SetKwSty{text}
        \DontPrintSemicolon
        \SetArgSty{text}
        \SetProgSty{text}
        \SetKw{KwIn}{in}

        \SetKwProg{Fn}{def}{}{}

        \Fn{get\_vectors(graphs)}{
            documents = [ ]\;
            \ForAll{graph \KwIn graphs}{
                walks = generate\_walks(graph)\;
                document = walks\_to\_document(walks)\;
                documents.append(document)\;
            }

            model = Text\_Embedding\_Model()\;
            model.fit(documents)\;
            \Return model.get\_document\_vectors()\;
        }

        \caption{basic idea of our walk based embedding}
        \label{algorithm:basic_idea_walk_to_vector}
    \end{algorithm}
\end{minipage}


\subsection{walk generation}
We have considered two approaches for generating walks: All Pair Shortest Paths (APSP) and Random Walks. Each method possesses distinct advantages and drawbacks. APSP tends to have more hotspots of frequently visited nodes, which may introduce bias in the document; however, it generates similar walks for structurally analogous graphs. On the other hand, Random Walks can be set to a specific length and do not exhibit the same issue with frequently visited nodes. Nevertheless, they do not guarantee the generation of similar walks for structurally similar graphs.

In our initial approach, we employed the all-pair shortest paths method; however, we quickly discovered that this was impractical for large datasets, as the sheer volume of text data overwhelmed the document embedding technique. Consequently, we opted for an alternative solution that limits the number of walks for each graph to a parameter referred to as sample\_size in the code.

By performing the random selection of two nodes sample\_size times, we obtain the shortest path between them and add it to the list of walks. Assuming that sample\_size is sufficiently large, it is reasonable to expect that every node will be visited multiple times. This alternative approach offers a more manageable solution when working with extensive datasets.

\subsection{Limits of information}
With an adequate quantity of walks over a graph, one can partially reconstruct the graph. However, there are limitations: For example, distinguishing between 'line' and 'circle' subgraphs is not always possible without any specific preparation of the graph. To illustrate this, refer to Figure \ref{figure:random_walk_cant_differentiate}. Both graphs consists of nodes that have the same feature, one is a circle, the other one a line. Each walk over one of the two graphs can be accommodated within the other graph as well. This hold true for random walks and for shortest paths.

\begin{figure}[ht!]
    \begin{minipage}{\linewidth}
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node (A) at (0,0) {0};
                    \node (B) at (0,2) {0};
                    \node (C) at (5,2) {0};
                    \node (D) at (5,0) {0};
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (B);
                    \path [-] (A) edge node {} (D);
                    \path [-] (C) edge node {} (B);
                    \path [-] (C) edge node {} (D);
                \end{scope}
            \end{tikzpicture}
        \end{minipage}%
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node (A) at (0,0) {0};
                    \node (C) at (5,2) {0};
                    \node (D) at (5,0) {0};
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (D);
                    \path [-] (C) edge node {} (D);
                \end{scope}
            \end{tikzpicture}
        \end{minipage}%
    \end{minipage}
    \caption[short]{Example of two graphs for whom each walk done one graph could also be done on the other}
    \label{figure:random_walk_cant_differentiate}
\end{figure}

These walks also have a hard time to give us information about the size of graph. Lets look at the example in figure \ref{figure:random_walk_cant_differentiate_regular}. All shortest paths as well as all random walks on one graph could also be done on the other. As a result, due to these inherent limitations, we did not expect walk-based whole graph embeddings to produce outstanding results.

\begin{figure}[ht!]
    \begin{minipage}{\linewidth}
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node (A) at (0,0) {0};
                    \node (B) at (0,2) {0};
                    \node (C) at (2,0) {0};
                    \node (D) at (2,2) {0};
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (D);
                    \path [-] (B) edge node {} (A);
                    \path [-] (C) edge node {} (B);
                    \path [-] (D) edge node {} (C);
                    \path [-] (A) edge node {} (C);
                    \path [-] (B) edge node {} (D);
                \end{scope}
            \end{tikzpicture}
        \end{minipage}%
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node (A) at (0,0) {0};
                    \node (B) at (0,2) {0};
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (B);
                \end{scope}
            \end{tikzpicture}

        \end{minipage}%
    \end{minipage}
    \caption[short]{Another example of two graphs for whom each walk done one graph could also be done on the other}
    \label{figure:random_walk_cant_differentiate_regular}
\end{figure}



\subsubsection{text embedding}
Throughout the entirety of our project, we employed Doc2Vec\cite{2014doc2vec} for transforming our walks into vector representations. Doc2Vec facilitates the training of a custom model using our specific vocabulary, allowing for the embedding of documents into vectors within this vocabulary. Despite our efforts to identify an alternative document-to-vector model that was user-friendly, we were unable to locate a suitable candidate.

While Doc2Vec is easy to use, we don't use it the way it is intended to be used. As natural language and our document 'language' certainly differ, we expect that there is a better method to encode our documents. Doc2Vec generates word vectors by trying to predict the surroundings of the word (or the word by its surroundings). In natural language processing it makes sense to define the surroundings over the boundaries of sentences, but in our case, a new path/sentence has no semantic connection to the last one.

%TODO Ein super langer random walk

\subsubsection{downstream training}
For downstream training of the regressor or classifier, we followed scikit-learn's recommendations and chose Support Vector Regression (SVR) or Support Vector Classification (SVC) to train and generate the label prediction, given the vector generated by our method. In the case of multi-output prediction, we employed scikit-learn's multi-output methods but sticked to the mentioned regressors and classifiers for the individual task. To replace missing values in the training data, we utilized scikit-learn's SimpleImputer.

\subsubsection{results}
Given the significant amount of data generated by random walks, which subsequently resulted in an extensive volume of training data for Doc2Vec, we focused on smaller datasets from the Open Graph Benchmark (OGB). For datasets with fewer than 10,000 graphs, we ran our model ten times; for those with less than 100,000 graphs, we executed it three times; and for larger datasets, we carried out a single run. The outcomes are presented in Table \ref{table:ogbg-molfreesolv_results}.
%TODO Zahlen korrekt?

%For most datasets, our methodology did not yield significant outcomes for the majority of the datasets. To validate this observation, we generated random vectors with dimensions equivalent to those produced by our model and employed these vectors in identical downstream training processes. These random vectors serve as a benchmark for evaluating our model's performance. To assess the probability that the observed sample sets are derived from the same underlying distribution (i.e., both are random), we performed a Kolmogorov-Smirnov test on the final results of our model and the predictions based on random vectors.

\begin{table}[h!]
    \centering
    \begin{tabular}{@{}lllllll@{}}
        \toprule
        name             & metric & Paths2Vec       & random          & ks-test p-value \\ \midrule
        ogbg-molclintox  & rocauc & 0.50 $\pm$ 0.00 & 0.50 $\pm$ 0.50 & 1.00000         \\
        ogbg-molmuv      & ap     & 0.00 $\pm$ 0.00 & 0.00 $\pm$ 0.00 & 1.00000         \\
        ogbg-moltox21    & rocauc & 0.50 $\pm$ 0.00 & 0.50 $\pm$ 0.50 & 1.00000         \\
        ogbg-molhiv      & rocauc & 0.51 $\pm$ 0.01 & 0.50 $\pm$ 0.50 & 0.60000         \\
        ogbg-molsider    & rocauc & 0.51 $\pm$ 0.00 & 0.50 $\pm$ 0.50 & 0.00022         \\
        ogbg-molbace     & rocauc & 0.64 $\pm$ 0.04 & 0.49 $\pm$ 0.49 & 0.00001         \\
        ogbg-molbbbp     & rocauc & 0.75 $\pm$ 0.01 & 0.50 $\pm$ 0.50 & 0.00001         \\
        ogbg-molesol     & rmse   & 1.49 $\pm$ 0.02 & 2.29 $\pm$ 2.29 & 0.00001         \\
        ogbg-molfreesolv & rmse   & 5.79 $\pm$ 0.08 & 6.77 $\pm$ 6.77 & 0.00001         \\
        ogbg-mollipo     & rmse   & 1.13 $\pm$ 0.01 & 1.25 $\pm$ 1.25 & 0.00001         \\
        ogbg-moltoxcast  & rocauc & 0.50 $\pm$ 0.00 & 0.50 $\pm$ 0.50 & 0.00001         \\ \bottomrule
    \end{tabular}
    \caption{comparison of our methods against random vectors}
    \label{table:ogbg-molfreesolv_results}
\end{table}

\subsubsection{conclusion}
We guess that the observed suboptimal performance may be attributed to the fact that our method is only capable of capturing certain types of information from the graph. As previously discussed, walk-based embeddings encounter difficulties in accurately representing the structural aspects of a graph, such as size and subgraphs. Our interpretation of these results is, that our model is able to capture the composition of the graph, specifically the relationships between different types of nodes. However, due to our limited knowledge about the datasets, we are unable to judge whether this hypothesis is consistent with the tasks associated with these datasets.

Some of the classification datasets are also quite unbalanced, as can be seen in table \ref{table:ogb_mol_datasets}, which make them harder to train on. Regression datasets in a way are easier to train on, as the they benefit even from a slight increase in information in the produced vectors.

\subsubsection{outlook}
We don't think that our approach by itself will generate good results for whole graph embeddings. Nevertheless, it might be interesting to search for a reason, why our method produces meaningful results on some datasets but not on others. This might give insight into the dataset and the task to solve but also give insight into graph embeddings in general.

It might me also interesting to change out Doc2Vec for another, maybe faster method that offers the possibility to run the ogb large scale graph embedding dataset 'PCQM4Mv2'. Instead of concatenating shortest paths, it might also be interesting to use one, continuous, random walk. This would also solve the problem of the windows\_size of Doc2Vec.

%https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
