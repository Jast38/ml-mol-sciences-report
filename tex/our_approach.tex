\section{Our approach}
To bolster our understanding, we decided to implement a graph machine learning method ourselves. Recognizing that it would be unlikely for us to devise a groundbreaking approach, we aimed to explore the simplest method we could imagine: a walk-based embedding.

For our walk-based embedding we focused on the embedding of whole graphs, as a similar method to embed nodes already exists (see \cite{2016node2vec}). One of the primary challenges in graph machine learning, as opposed to text-based machine learning, is the multidimensionality of graphs. Each node can have varying numbers of neighbors and different features and feature types. Walk based embeddings reduce this multidimensionality.

One reason to consider using graph walks for analysis is that, given a sufficient number of walks over a graph, it becomes possible to reconstruct the original graph to some degree\cite{Wittmann2009reconstruction}. These walks therefore provide insight into the graph's structure, capturing information about the relationships between vertices and the paths connecting them. By aggregating data from numerous walks, a comprehensive representation of the graph can be formed, enabling its reconstruction for further evaluation.

Our method can be described at a high level as follows: Given a graph, we generate a list of walks over the graph, in which the node IDs are substituted with the corresponding node features. This process results in a text document-like representation of the graph. Utilizing natural language processing techniques, we then transform the text document into a vector representation. These vector representations can subsequently be employed to train a model capable of predicting various graph characteristics.

\subsection{Walk based graph embeddings}
A walk on graph is a unambiguously sequence of vertices and edges, where each vertex is connected to the next vertex by an edge. Given a graph $g$, we can write a walk as list of nodes (denoted by their id), e.g. $(0, 2, 3)$ is a walk that start from the node with the id $0$, goes to $2$ and end at $3$. For a non-trivial graph there a many different possible walks, therefore we can create a set of walks $w$ that all walk on the same graph. At this moment the walk doesn't contain any information relevant for our task, for this we need to substitute the node ids with the corresponding features. We substitute each id with the feature value prefixed with index of the feature separated by an otherwise unused character. For this we require that the features of a node are ordered. Therefore our schema for a feature in the walk is FeatureIndex\_FeatureValue. We use this prefix in order to differentiate between same values of different feature types.

\begin{figure}[ht!]
    \begin{minipage}{\linewidth}
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node[draw=red] (A) at (0,0) {0\\(0, 2, 2)};
                    \node (B) at (0,4) {1\\(7, 0, 4)};
                    \node (C) at (2.5,4) {2\\(2, 0, 4)};
                    \node[draw=red] (D) at (2.5,2) {3\\(8, 3, 9)};
                    \node (E) at (5, 0) {4\\(3, 4, 7)};
                    \node[draw=red] (F) at (5,4) {5\\(2, 1, 7)} ;
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (B);
                    \path [-] (B) edge node {} (C);
                    \path [-, draw=red] (A) edge node {} (D);
                    \path [-] (D) edge node {} (C);
                    \path [-] (A) edge node {} (E);
                    \path [-] (D) edge node {} (E);
                    \path [-, draw=red] (D) edge node {} (F);
                    \path [-] (C) edge node {} (F);
                    \path [-] (E) edge node {} (F);
                \end{scope}
            \end{tikzpicture}
        \end{minipage}%
        \begin{minipage}{.5\linewidth}
            \centering
            (0, 3, 5) \\
            $\Downarrow$ \\
            (0\_0 1\_2 2\_2) (0\_8 1\_3 2\_9) (0\_2 1\_1 2\_7) \\
            $\Downarrow$ \\
            0\_0 1\_2 2\_2 0\_8 1\_3 2\_9 0\_2 1\_1 2\_7
        \end{minipage}
    \end{minipage}
    \caption[short]{Example of graph on which a walk is done that is converted into a sentence}
    \label{figure:example_walk_to_sentence}
\end{figure}

Lets talk about a concrete example. In the left side of figure \ref{figure:example_walk_to_sentence} is an undirected graph with six vertices given. Each vertex has three numerical features. These features are given as list of integers below the node id. The red marked nodes form a walk from node 0 to node 5. On the right side of the figure, we listed the steps needed for each walk to generate the document needed for the downstream training: First we generate a walk, then we substitute the node id with the prefixed features and at last we concatenate the features into a sentence.

Now we can formalize this method. Given a set of graphs $G = {g_i \mid i \in 0 \dots n }$, where $n$ is the number of graphs, we can generate a set of sets of walks $W = {\ w_i \mid i \in 0 \dots n }$. Next, we create a new set $W'$ in which we substitute the node ID with the node features for each walk. If we consider each walk as a sentence, we obtain a set of documents (comprising multiple sentences). These documents are then fed into a text embedding method. The pseudocode for our initial implementation of the graph embedding method is provided in algorithm \ref{algorithm:basic_idea_walk_to_vector}.

\begin{minipage}{\linewidth}
    \begin{algorithm}[H]
        %\SetKwSty{text}
        \DontPrintSemicolon
        \SetArgSty{text}
        \SetProgSty{text}
        \SetKw{KwIn}{in}

        \SetKwProg{Fn}{def}{}{}

        \Fn{get\_vectors(graphs)}{
            documents = [ ]\;
            \ForAll{graph \KwIn graphs}{
                walks = generate\_walks(graph)\;
                document = walks\_to\_document(walks)\;
                documents.append(document)\;
            }

            model = Text\_Embedding\_Model()\;
            model.fit(documents)\;
            \Return model.get\_document\_vectors()\;
        }

        \caption{basic idea of our walk based embedding}
        \label{algorithm:basic_idea_walk_to_vector}
    \end{algorithm}
\end{minipage}


\subsection{walk generation}
We have considered two approaches for generating walks: All Pair Shortest Paths (APSP) and Random Walks. Each method possesses distinct advantages and drawbacks. APSP tends to have more hotspots of frequently visited graphs, which may introduce bias in the document; however, it generates similar walks for structurally analogous graphs. On the other hand, Random Walks can be set to a specific length and do not exhibit the same issue with frequently visited graphs. Nevertheless, they do not guarantee the generation of similar walks for structurally similar graphs.

In our initial approach, we employed the all-pair shortest paths method; however, we quickly discovered that this was impractical for large datasets, as the sheer volume of text data overwhelmed the document embedding technique. Consequently, we opted for an alternative solution that limits the number of walks for each graph to a parameter referred to as sample\_size in the code.

By performing the random selection of two nodes sample\_size times, we obtain the shortest path between them and add it to the list of walks. Assuming the sample\_size is sufficiently large, it is reasonable to expect that every node will be visited multiple times. This alternative approach offers a more manageable solution when working with extensive datasets.

\paragraph{Reconstructing a graph from walks}
With an adequate quantity of walks over a graph, one can partially reconstruct the graph. However, there are limitations: For example, distinguishing between 'line' and 'circle' subgraphs is not always possible without any specific preparation of the graph. To illustrate this, refer to Figure \ref{figure:random_walk_cant_differentiate}. It is evident that each potential walk over one two graphs can be accommodated within the other graph as well.

\begin{figure}[ht!]
    \begin{minipage}{\linewidth}
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node (A) at (0,0) {0};
                    \node (B) at (0,2) {0};
                    \node (C) at (5,2) {0};
                    \node (D) at (5,0) {0};
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (B);
                    \path [-] (A) edge node {} (D);
                    \path [-] (C) edge node {} (B);
                    \path [-] (C) edge node {} (D);
                \end{scope}
            \end{tikzpicture}
        \end{minipage}%
        \begin{minipage}{.5\linewidth}
            \centering
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw, align=center}]
                    \node (A) at (0,0) {0};
                    \node (B) at (0,2) {0};
                    \node (C) at (5,2) {0};
                    \node (D) at (5,0) {0};
                \end{scope}

                \begin{scope}[>={Stealth[black]},
                    every node/.style={circle},
                    every edge/.style={draw, very thick}]
                    \path [-] (A) edge node {} (B);
                    \path [-] (A) edge node {} (D);
                    \path [-] (C) edge node {} (D);
                \end{scope}
            \end{tikzpicture}
        \end{minipage}%
    \end{minipage}
    \caption[short]{Example of two graphs for whom each walk done one graph could also be done on the other}
    \label{figure:random_walk_cant_differentiate}
\end{figure}

In the case of regular graphs, where all vertices share the same degree, nodes with identical features often appear indistinguishable during walks. As a result, due to these inherent limitations, we did not expect walk-based whole graph embeddings to produce outstanding results for such graphs.

\subsubsection{text embedding}
Throughout the entirety of our project, we employed Doc2Vec\cite{2014doc2vec} for transforming our walks into vector representations. Doc2Vec facilitates the training of a custom model using our specific vocabulary, allowing for the embedding of documents into vectors within this vocabulary. Despite our efforts to identify an alternative document-to-vector model that was user-friendly, we were unable to locate a suitable candidate.

While Doc2Vec is easy to use, we don't use it the way it is intended to be used. As natural language and our document 'language' certainly differ, we expect that there is a better to encode our documents. Doc2Vec generates word vectors by trying to predict the surroundings of the word (or the word by its surroundings). In natural language processing it makes sense to define the surroundings over the boundaries of sentences, but in our case, a new path/sentence has no semantic connection to the last one.

%TODO Ein super langer random walk

\subsubsection{downstream training}
For downstream training of the regressor or classifier, we followed scikit-learn's recommendations and chose Support Vector Regression (SVR) or Support Vector Classification (SVC) to train and generate the label prediction given the vector generated by out method. In the case of multi-output prediction, we employed scikit-learn's multi-output methods but sticked to the mentioned regressors and classifiers for the individual task. To replace missing values in the training data, we utilized scikit-learn's SimpleImputer.

\subsubsection{results}
Given the significant amount of data generated by random walks, which subsequently resulted in an extensive volume of training data for Doc2Vec, we focused on smaller datasets from the Open Graph Benchmark (OGB). For datasets with fewer than 10,000 graphs, we ran our model ten times; for those with less than 100,000 graphs, we executed it three times; and for larger datasets, we carried out a single run. The outcomes are presented in Table \ref{table:ogbg-molfreesolv_results}.
%TODO Zahlen korrekt?

For most datasets, our methodology did not yield significant outcomes for the majority of the datasets. To validate this observation, we generated random vectors with dimensions equivalent to those produced by our model and employed these vectors in identical downstream training processes. These random vectors serve as a benchmark for evaluating our model's performance. To assess the probability that the observed sample sets are derived from the same underlying distribution (i.e., both are random), we performed a Kolmogorov-Smirnov test on the final results of our model and the predictions based on random vectors.

%TODO Korrelation unbalanced and schlechte Ergebnisse

\begin{table}[h!]
    \centering
    \begin{tabular}{@{}lrrrrrr@{}}
        \toprule
        dataset          & \multicolumn{2}{c}{paths2vec} & \multicolumn{2}{c}{random vectors} & \multicolumn{2}{c}{t-test}                         \\ \cmidrule(l){2-3}\cmidrule(l){4-5}\cmidrule(l){6-7}
                         & mean                          & std                                & mean                       & std    & D & p-value  \\ \midrule
        ogbg-molesol     & 1.4651                        & 0.0309                             & 2.2844                     & 0.0239 & 1 & 1.08E-05 \\
        ogbg-molfreesolv & 5.7668                        & 0.0557                             & 6.7620                     & 0.0296 & 1 & 1.08E-05 \\
        ogbg-mollipo     & TODO                          & TODO                               & TODO                       & TODO   &   &          \\ \bottomrule
    \end{tabular}
    \caption{comparison of the rmse for 10 runs on the dataset regression datasets}
    \label{table:ogbg-molfreesolv_results}
\end{table}

\subsubsection{conclusion}
We guess that the observed suboptimal performance may be attributed to the fact that our methods are only capable of capturing certain types of information from the graph. As previously discussed, walk-based embeddings encounter difficulties in accurately representing the structural aspects of a graph, such as size and subgraphs. Our interpretation of these results is, that our model is able to capture the composition of the graph, specifically the relationships between different types of nodes. However, due to our limited knowledge about the datasets, we are unable to judge whether this hypothesis is consistent with the tasks associated with these datasets.

Some of the classification datasets are also quite unbalanced, as can be seen in table \ref{table:ogb_mol_datasets}, which make them harder to train on. Regression datasets in a way are easier to train on, as the they benefit even from a slight increase in information in the produced vectors.

\subsubsection{outlook}
We don't think that our approach by itself will generate good results for whole graph embeddings. Nevertheless, it might be interesting to search for a reason, why our methods produces meaningful results on some datasets but not on others. This might give insight into the dataset and the task to solve but also give insight into graph embeddings in general.

It might me also interesting to change out Doc2Vec for another, maybe faster method that offers the possibility to run the ogb large scale graph embedding dataset 'PCQM4Mv2'. Instead of concatenating shortest paths, it might also be interesting to use one, continuous, random walk. This would also solve the problem of the windows\_size of Doc2Vec.

%https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
